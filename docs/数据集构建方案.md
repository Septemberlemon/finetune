```Mermaid
graph TD
    Start["问题/目标:<br>用LCCC数据<br>提升RP模型拟人化"] --> LCCC_Data[(LCCC Data)];
    Start --> Existing_RP_Data(可选: 现有<br>高质量RP数据);
    Start --> Base_Model(基础预训练<br>语言模型);

    subgraph "方案一: 反向生成System Prompt"
        direction TB
        S1_1[1a. 准备LCCC子集]
        S1_2[1b. 准备带Profile的RP数据];
        S1_3[1c. 训练角色卡生成模型<br>Input: 对话<br>Output: Profile];
        S1_4[1d. 为LCCC子集生成Profile];
        S1_5[1e. 组合: 生成的Profile<br>+ LCCC对话];
        S1_6[1f. SFT训练<br>角色扮演模型];

        LCCC_Data --> S1_1;
        Existing_RP_Data --> S1_2;
        S1_2 --> S1_3;
        S1_1 --> S1_4;
        S1_3 --> S1_4;
        S1_4 --> S1_5;
        Base_Model --> S1_6;
        S1_5 --> S1_6;
    end

    subgraph "方案二: Staging Train"
        direction TB
        S2_1[2a. LCCC数据预处理<br>拼接, Masking, 通用Prompt];
        S2_2[2b. Stage 0 训练<br>在处理后LCCC上训练];
        S2_3[2c. 获取Stage 0模型];
        S2_4[2d. 准备标准RP数据];
        S2_5[2e. Stage 1 SFT训练<br>在标准RP数据上训练];

        LCCC_Data --> S2_1;
        Base_Model --> S2_2;
        S2_1 --> S2_2;
        S2_2 --> S2_3;
        Existing_RP_Data --> S2_4;
        S2_3 --> S2_5;
        S2_4 --> S2_5;
    end

    subgraph "方案三: 基于改写器的数据合成Pipeline"
        direction TB
        S3_1[3a. 生成基础RP数据<br>角色卡+对话, AI味];
        S3_2[3b. 获取LCCC数据];
        S3_3[3c. 生成'去拟人化'数据对<br>LCCC 改写为 AI味];
        S3_4[3d. 清洗数据对<br>语义一致性检测];
        S3_5[3e. 训练'拟人化改写器'<br>Input: AI味句子<br>Output: LCCC句子];
        S3_6[3f. 获取改写器模型];
        S3_7[3g. 应用改写器<br>改写基础RP数据的回复];
        S3_8[3h. 严格清洗改写结果<br>语义, OoC等检测];
        S3_9[3i. 组合最终合成数据];
        S3_10[3j. SFT训练<br>角色扮演模型];

        Start --> S3_1;
        LCCC_Data --> S3_2;
        S3_2 --> S3_3;
        S3_3 --> S3_4;
        S3_4 --> S3_5;
        S3_5 --> S3_6;
        S3_1 --> S3_7;
        S3_6 --> S3_7;
        S3_7 --> S3_8;
        S3_8 --> S3_9;
        S3_1 --> S3_9;
        Base_Model --> S3_10;
        S3_9 --> S3_10;
    end

    S1_6 --> End_Goal["最终: 提升了<br>拟人化能力的<br>角色扮演模型"];
    S2_5 --> End_Goal;
    S3_10 --> End_Goal;

    %% Styling (Optional)
    style S1_3 fill:#e0f2f1,stroke:#000,stroke-width:2px,color:#000
    style S1_6 fill:#ffe0b2,stroke:#000,stroke-width:2px,color:#000
    style S2_2 fill:#e0f2f1,stroke:#000,stroke-width:2px,color:#000
    style S2_5 fill:#ffe0b2,stroke:#000,stroke-width:2px,color:#000
    style S3_5 fill:#e0f2f1,stroke:#000,stroke-width:2px,color:#000
    style S3_8 fill:#ffe0b2,stroke:#000,stroke-width:2px,color:#000
    style S3_10 fill:#ffe0b2,stroke:#000,stroke-width:2px,color:#000
    style End_Goal fill:#bbdefb,stroke:#000,stroke-width:2px,color:#000
```
---


## **步骤实现文档：利用LCCC数据提升角色扮演模型拟人化能力**
[原文](https://zhuanlan.zhihu.com/p/719772276)  
**背景:**

我们面临一个挑战：如何在缺乏大量精标注角色扮演数据的情况下，显著提升模型的“拟人化”程度，使其回复更像真人，甚至达到以假乱真的效果。直接使用LCCC（Large-scale Clean Chinese Conversations）这类真人对话数据进行SFT（Supervised Fine-Tuning）存在诸多问题，例如：

1.  **缺乏角色设定 (Profile/System Prompt):** 导致模型遵循特定角色设定的能力下降。
2.  **幻觉 (Hallucination):** 没有Profile约束，模型可能基于对话内容产生与设定不符的信息。
3.  **风格同质化:** 海量随意的真人对话可能覆盖模型原有的多样性，使所有角色的说话风格趋于一致（例如都变得活泼），这与某些角色（如沉默寡言）的设定相悖。

本文档将详细介绍三种尝试解决这些问题、有效利用LCCC数据的方案。

---

### **方案一：为LCCC数据反向生成System Prompt**

**核心思想:** 既然LCCC数据缺少角色设定，我们就利用已有的、带设定的角色扮演数据训练一个模型，让它学会根据对话反推出可能的角色设定，然后将生成的设定与LCCC对话配对，形成新的训练数据。

**实现步骤:**

1.  **准备高质量LCCC子集:**
    *   **目标:** 从海量的LCCC数据中筛选出质量高、适合模拟真实对话风格的部分。
    *   **方法:**
        *   进行数据清洗，去除明显的badcase（如人类难以理解的对话）。
        *   可以采用聚类分析或语义分析等方法，识别并选取高质量的对话片段。
        *   **注意:** 无需使用全部LCCC数据，选取高质量子集即可，避免信息冗余和训练效率低下。

2.  **准备高质量、多样化的带Profile的角色扮演数据:**
    *   **目标:** 用于训练“角色卡生成模型”。
    *   **要求:**
        *   **高质量对话:** 这部分数据的对话本身应尽可能拟人化，使训练数据分布与后续LCCC推理数据的分布更接近。
        *   **多样化角色卡:** 包含各种性格、背景、关系的设定，防止生成模型过拟合，能泛化到不同的对话风格。

3.  **训练角色卡生成模型 (反向训练):**
    *   **输入:** 从步骤2准备的数据中提取 **对话上下文**。
    *   **输出 (标签):** 对应的 **角色卡 (System Prompt)**。
    *   **模型:** 训练一个序列到序列（Seq2Seq）模型（如基于Transformer的编码器-解码器架构）。
    *   **训练目标:** 让模型学会根据对话内容，生成一个合理的、描述发言者特征的角色卡。

4.  **为LCCC子集生成角色卡:**
    *   **输入:** 将步骤1筛选出的LCCC对话片段输入到步骤3训练好的角色卡生成模型中。
    *   **输出:** 模型为每一段LCCC对话生成对应的角色卡。

5.  **构建最终训练数据集:**
    *   将步骤4生成的角色卡与其对应的LCCC对话片段组合起来。
    *   **格式:** `(System Prompt: 生成的角色卡, 对话: LCCC对话)`

6.  **训练角色扮演模型:**
    *   使用步骤5构建的数据集，按照标准的角色扮演SFT流程训练你的目标对话模型。
    *   **训练方式:** 给定System Prompt（生成的角色卡）和用户输入，模型学习生成对应的角色回复（只计算角色回复部分的Loss）。

**关键注意事项与风险:**

*   **信息量不对等:** 这是此方案的核心缺陷。生成的角色卡信息量可能 **小于** 对话中蕴含的信息量（如示例中对话提到“姐姐”，但生成卡未体现年龄关系）。
    *   **风险:** 训练时，模型可能学会在对话信息超出角色卡范围时“自由发挥”，导致推理时产生幻觉。
    *   **缓解策略:**
        *   尽可能提升角色卡生成模型的质量，使其捕捉更多对话细节。
        *   在生成角色卡后，进行人工或自动校验，尝试补充缺失的关键信息，或过滤掉信息不对等严重的样本。
        *   理想状态是追求“角色卡信息量 >= 对话信息量”，尽量减少“画蛇添足”的生成。
*   **数据质量:** 角色卡生成模型的训练数据（步骤2）质量至关重要。低质量数据会导致生成效果差。
*   **LCCC筛选:** LCCC数据的筛选标准直接影响最终效果。过于随意或低质的对话即使配上角色卡，也可能效果不佳。

---

### **方案二：Staging Train (分阶段训练)**

**核心思想:** 不直接将LCCC用于SFT，而是在SFT之前增加一个“预热”阶段，让模型“浏览”LCCC数据，学习其说话风格，但不强求其严格遵循问答或角色扮演格式。

**实现步骤:**

1.  **准备LCCC数据:**
    *   同样建议进行初步的质量筛选。

2.  **数据拼接与格式化:**
    *   **目标:** 解决LCCC数据轮次短、数量多的问题，提高训练效率，减少短文本偏见和灾难性遗忘。
    *   **方法:**
        *   将多个LCCC对话session **拼接** 成更长的序列，直至达到模型允许的最大上下文窗口长度（`max_length`）。
        *   **Loss计算调整:**
            *   在拼接后的长序列中，对于 **每个原始session的第一个发言者** 的内容（通常是提问或发起对话的部分），**不计算Loss**。这可以避免因拼接带来的上下文逻辑矛盾影响模型学习。
            *   原始session中 **后续的所有对话内容** (通常是回复) **参与Loss计算**。
        *   **添加通用System Prompt:** 在拼接数据的开头加入一个通用的、非特定角色的System Prompt，例如：“你现在是一个角色扮演专家。”或类似的指令。
            *   **目的:** 建立System Prompt隔离机制，帮助模型在后续推理时区分这种“风格学习”阶段和实际的角色扮演任务，使其更容易在需要时链接到LCCC的说话风格。

3.  **执行Staging Train (Stage 0):**
    *   **模型:** 使用你的 **预训练模型 (Pre-trained Model)** 作为基础。
    *   **数据:** 使用步骤2处理好的拼接LCCC数据。
    *   **训练方式:** 类似于SFT，计算Answer部分的Loss（根据步骤2.b的规则）。
    *   **训练时长:** 通常 **仅需1个epoch** 左右。避免过度训练导致遗忘或风格固化。
    *   **(可选) 混合训练:** 可以在这个阶段掺杂少量其他任务数据（如小说续写、通用SFT数据）来维持模型通用能力。

4.  **执行标准SFT (Stage 1+):**
    *   **模型:** 使用 **经过Stage 0训练后的模型**。
    *   **数据:** 使用你 **真正的、带Profile的角色扮演数据集**。
    *   **训练方式:** 进行标准的角色扮演SFT。

**关键注意事项与风险:**

*   **能力融合的不确定性:** 期望模型融合角色扮演能力(A)和LCCC拟人风格(B)，但结果可能不如预期，可能得到一个“两者皆弱”的模型。
*   **知识遗忘:** Stage 0 可能导致预训练模型的部分知识被遗忘。
*   **能力遗忘:** 后续Stage 1 的SFT也可能覆盖掉Stage 0学到的拟人风格。 (实验表明此风险相对较小)
*   **智力指标下降:** 采用此策略的模型在一些通用评估榜单（如Align Bench, IFEval）上可能出现性能退化。
*   **拼接逻辑:** 确保拼接和Loss Masking的实现正确，避免引入错误的学习信号。

---

### **方案三：基于改写器的数据合成Pipeline**

**核心思想:** 首先用常规方法（如GPT生成）构建基础的角色扮演数据集，然后训练一个“拟人化改写器”（利用LCCC数据间接训练），用这个改写器将基础数据集中“AI味”较重的角色回复改写得更像人话，最后进行严格清洗，得到高质量的合成数据用于训练。

**实现步骤:**

**Part 1: 生成基础角色扮演数据**

1.  **获取角色卡:**
    *   **来源:**
        *   通过Few-shot Prompting让GPT生成。 (注意控制temperature以平衡多样性与事实性)
        *   爬取Wiki等百科信息，再用GPT改写成Prompt格式。
        *   使用Hugging Face等平台的开源角色数据集。
    *   **建议:** 混合使用多种来源，增加多样性。

2.  **生成对话内容:**
    *   **输入:** 角色卡 + (可选) 手工构建的少量启动对话。
    *   **方法 (Few-shot Prompting):**
        *   **方法A (单次生成):** 让GPT一次性生成整个对话session。 (回复通常较短)
        *   **方法B (多轮调用):** 先构建用户问题，然后让GPT扮演指定角色生成回复，重复此过程。 (类似Baize/PIPPA，通常效果更好，但更易产生AI味回复)

3.  **初步清洗:**
    *   对生成的角色卡和对话进行基本的格式检查和内容过滤。
    *   **产出:** 一个基础的角色扮演数据集，但角色回复可能不够拟人化。

**Part 2: 构建“拟人化改写器”**

4.  **生成“去拟人化”数据对:**
    *   **核心策略 (左脚踩右脚):** 利用GPT擅长将“人话”改成“AI话”的能力。
    *   **输入:** LCCC数据集中的 **真实人类对话句子** (拟人化的句子)。
    *   **任务:** 使用精心设计的Prompt (参考原文提供的模板)，让GPT将输入的LCCC句子 **改写成更书面化、礼貌、刻板的“AI味”风格**，同时保持原意不变。
        *   **Prompt关键点:** 强调保持语义、实体词、格式不变，仅改写风格。
    *   **输出:** `(原LCCC句子, GPT改写的AI味句子)` 数据对。

5.  **清洗“去拟人化”数据对 (语义一致性检测):**
    *   **目标:** 确保GPT的改写没有改变原意、添加/遗漏信息或弄错实体。
    *   **方法:** 使用另一个Prompt (参考原文提供的模板)，让GPT或另一个模型判断步骤4生成的 `(原句, 改写句)` 对的语义是否严格一致。
        *   **Prompt关键点:** 定义严格的“语义相同”标准，明确何种情况算False (如信息增删、实体错误、语义偏差)。
    *   **筛选:** 只保留判断结果为True (语义一致) 的数据对。

6.  **训练“拟人化改写器”:**
    *   **数据:** 使用步骤5筛选后的 **高质量语义一致数据对**。
    *   **训练任务 (反向):**
        *   **输入:** **GPT改写的AI味句子**。
        *   **输出 (标签):** **原始的LCCC句子** (人话)。
    *   **模型:** 训练一个本地的Seq2Seq模型。
    *   **目标:** 让模型学会将输入的“AI味”句子改写回“人话”风格。

**Part 3: 应用改写器并最终清洗**

7.  **改写基础数据的角色回复:**
    *   **输入:** 将步骤3产出的基础角色扮演数据集中，**所有角色的回复 (Character Response)** 逐条输入到步骤6训练好的“拟人化改写器”中。
    *   **输出:** 经过改写器处理的、更具“人话”风格的角色回复。

8.  **对改写结果进行严格清洗:**
    *   **目标:** 解决改写器可能引入的错误、风格不一致问题。这是**极其关键**的一步。
    *   **清洗维度:**
        *   **语义一致性:** 检查改写后的回复是否仍与原始AI味回复的意图一致，是否遗漏关键信息或添加冗余信息 (可再次使用类似步骤5的检测方法)。
        *   **角色一致性 (OoC检测):**
            *   判断改写后的风格是否符合 **对应的角色卡设定**。例如，严肃角色不应出现过于随意的口语。
            *   方法: 可以用GPT进行人设一致性检测；对角色卡进行分类，对特定类型（如严肃、古代）的角色应用更严格的风格过滤；分析句子或n-gram特征剔除异常。
        *   **其他Badcase:** 剔除明显的语法错误、逻辑不通等问题。
    *   **策略:** 由于数据是合成的，理论上量很大，可以采用 **非常严格** 的清洗标准，宁缺毋滥。

9.  **组装最终数据集:**
    *   将 **原始角色卡**、**原始用户提问** 和 **经过改写且严格清洗后的角色回复** 重新组合成最终的训练数据。

10. **训练角色扮演模型:**
    *   使用步骤9得到的、高质量的、拟人化风格的合成数据集进行SFT训练。

**关键注意事项与风险:**

*   **改写器误差:** 改写器本身不可避免存在错误率，可能引入语义偏差、信息增删。
*   **清洗的重要性:** 步骤5和步骤8的清洗是成败关键。清洗不彻底会导致模型学到错误模式，产生幻觉、OoC或风格同质化。
*   **风格同质化风险:** 即使经过清洗，如果改写器本身风格单一，仍可能导致所有角色的最终说话风格趋同。需要关注改写器训练数据的多样性（源自LCCC的多样性）。
*   **成本:** 虽然数据合成量大，但多阶段的生成、训练、特别是严格的清洗过程（可能需要借助强力模型如GPT进行判断）仍然需要计算资源和时间成本。
*   **Pipeline复杂度:** 相比前两种方案，这是一个更复杂、环节更多的Pipeline，需要精心设计和调试每个步骤。


请根据您的具体情况（可用资源、模型基础、对风险的容忍度、期望达到的效果等）选择合适的方案进行尝试和优化。