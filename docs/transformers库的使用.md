# transformersæ˜¯huggingfaceç”Ÿæ€ä¸‹çš„æ ¸å¿ƒåº“ï¼Œç”¨äºåŠ è½½ã€è®­ç»ƒå’Œæ¨ç†ä¸»æµçš„Transformeræ¶æ„æ¨¡å‹

ä½¿ç”¨`pip install transformers`å³å¯å®‰è£…

***

### pipeline

ä½¿ç”¨`pipeline`åˆ›å»ºä¸€ä¸ªå¯¹è¯ç®¡é“ï¼Œå®ƒçš„ä½œç”¨æ˜¯å¿«é€Ÿè°ƒç”¨æ¨¡å‹è¿›è¡Œå¯¹è¯ï¼š

```python
from transformers import pipeline

generator = pipeline("text-generation", model="unsloth/Qwen3-14B")
```

ä¸Šè¿°ä»£ç ä¼šä»**Huggingface Hub**ä¸‹è½½æ¨¡å‹åˆ°æœ¬åœ°ï¼Œå¹¶åŠ è½½æ¨¡å‹ï¼Œå¾—åˆ°çš„`generator`æ˜¯ä¸€ä¸ª`Pipeline`å¯¹è±¡`pipeline`å‡½æ•°çš„ç¬¬ä¸€ä¸ªå‚æ•°æ˜¯`task`ï¼Œç”¨äºæŒ‡å®šå…·ä½“çš„äººç‰©ç±»å‹ï¼Œ`model`å‚æ•°ç”¨æ¥æŒ‡å®šå…·ä½“æ¨¡å‹åç§°ï¼Œå¯¹äºä»è¿œç¨‹ä¸‹è½½çš„æ¨¡å‹æ¥è¯´ï¼Œç›´æ¥æŒ‡å®šä¸ºå®˜ç½‘ä¸Šçš„æ¨¡å‹åç§°å³å¯

æ¥ä¸‹æ¥å¯ä»¥ç›´æ¥è°ƒç”¨`generator`æœ¬èº«æ¥è¿›è¡Œæ–‡æœ¬ç»­å†™ï¼š

```python
prompt = "ä»Šå¤©å¤©æ°”ä¸é”™"
results = generator(prompt, max_length=50)
print(results[0]["generated_text"])
```

å®ƒå¯èƒ½å¾—åˆ°ï¼š

```text
ä»Šå¤©å¤©æ°”ä¸é”™ï¼Œé˜³å…‰æ˜åªšï¼Œæˆ‘æ‰“ç®—å»å…¬å›­æ•£æ­¥ï¼Œé¡ºä¾¿ä¹°ç‚¹ä¸œè¥¿ã€‚æˆ‘å…ˆå»è¶…å¸‚ä¹°äº†äº›æ°´æœå’Œè”¬èœï¼Œç„¶åå»å…¬å›­ã€‚å…¬å›­é‡Œäººå¾ˆå¤šï¼Œæˆ‘ååœ¨é•¿æ¤…ä¸Šä¼‘æ¯ï¼Œçœ‹åˆ°å­©å­ä»¬åœ¨ç©è€ï¼Œæ„Ÿè§‰
```

`generator`æœ¬èº«æ¥æ”¶ä¸€ä¸ª`prompt`å‚æ•°å¤–åŠ è‹¥å¹²å…³é”®å­—å‚æ•°ï¼Œ`prompt`å‚æ•°å¯ä»¥æ˜¯ä¸€ä¸ª**str**æˆ–è€…**list[str]**ï¼Œå¯¹äºå‰è€…ï¼Œè°ƒç”¨ç»“æœå°†å¾—åˆ°ä¸€ä¸ª**list[dict]**ã€å¯¹äºåè€…ï¼Œè°ƒç”¨ç»“æœå°†å¾—åˆ°ä¸€ä¸ª**list[list[dict]]**ï¼Œä¾‹å¦‚ï¼š

```python
prompts = ["ä»Šå¤©å¤©æ°”ä¸é”™ï¼Œ", "æ³•å›½çš„é¦–éƒ½æ˜¯"]

results = generator(prompts, max_length=50, num_return_sequences=3)
```

å®ƒæ‹¿åˆ°çš„`results`æ˜¯ä¸€ä¸ªé•¿åº¦ä¸º**2**çš„**list**ï¼Œå…¶ä¸­ä¸¤ä¸ªå…ƒç´ åˆ†åˆ«ä»£è¡¨å¯¹`prompt`ä¸­ä¸¤æ®µæ–‡æœ¬ç»­å†™çš„ç»“æœ

`results`ä¸­çš„æ¯ä¸ªå…ƒç´ åˆæ˜¯ä¸€ä¸ªé•¿åº¦ä¸º**3**çš„**list**ï¼Œå…¶ä¸­æ¯ä¸ªå…ƒç´ ä»£è¡¨å¯¹ä¸€æ®µæ–‡æœ¬è¿›è¡Œç»­å†™çš„ä¸€ç§ç»“æœï¼Œè¿™æ˜¯å› ä¸ºæŒ‡å®šäº†`num_return_sequences=3`ï¼Œæ‰€ä»¥å¯¹ä¸€ä¸ªæ–‡æœ¬å°†ä¼šæœ‰ä¸‰ä¸ªç”Ÿæˆç»“æœ

æœ€å†…å±‚çš„æ˜¯ä¸€ä¸ª**python dict**ï¼Œä»£è¡¨å¯¹**ä¸€æ®µæ–‡æœ¬çš„ä¸€ä¸ªç”Ÿæˆç»“æœ**ï¼Œæ•´ä¸ª`results`ä¸­å…±æœ‰**6**ä¸ªè¿™æ ·çš„**dict**

é€šè¿‡ï¼š

```python
for result in results:
    for i in range(3):
        print(result[i]["generated_text"])
```

å³å¯æ‰“å°æ‰€æœ‰ç”Ÿæˆç»“æœ

å¯¹äº`prompt`ä¸ºå•ä¸ª**str**çš„æƒ…å†µï¼Œå®ƒå¾—åˆ°çš„ç»“æœåªæ˜¯å°‘äº†æœ€å¤–å±‚çš„å¯¹åº”ä¸åŒ**prompt**çš„ä¸€å±‚åˆ—è¡¨

ä¸‹é¢ä»‹ç»ä¸€äº›`Pipeline`å¯¹è±¡çš„å…¶ä»–è°ƒç”¨å‚æ•°ï¼š

`max_length`ï¼šå®ƒå†³å®šäº†æ–‡æœ¬çš„æœ€å¤§é•¿åº¦ï¼Œä¾‹å¦‚ä¼ è¿›å»çš„æ–‡æœ¬é•¿åº¦ä¸º**20**ä¸ª**token**ï¼Œ`max_length`ä¸º**50**ï¼Œåˆ™ä¼šå°†æ–‡æœ¬ç»­å†™åˆ°**50**ä¸ª**token**ï¼Œå¦‚æœæ²¡åˆ°**50**ä¸ª**token**é‡åˆ°ä¸­æ­¢**token**ä¹Ÿå°†åœæ­¢ç»­å†™

`max_new_tokens`ï¼šé™å®šæ–°ç”Ÿæˆçš„**token**çš„æœ€å¤§æ•°é‡

`temperature`ï¼šæ¸©åº¦ï¼Œç”¨äºæ§åˆ¶æ¨¡å‹å€¾å‘äº**æ›´ä¿å®ˆ**è¿˜æ˜¯**æ›´å…·åˆ›é€ æ€§**

`top_k`ï¼šé™å®šè¾“å‡ºæ—¶ä»æ¦‚ç‡æœ€é«˜çš„**k**ä¸ª**token**è¿›è¡Œé‡‡æ ·

`top_p`ï¼šè¾“å‡ºé‡‡æ ·æ—¶ä»æ¦‚ç‡æœ€é«˜çš„**token**å¼€å§‹ï¼Œä»é«˜åˆ°ä½ç´¯åŠ **token**çš„æ¦‚ç‡ï¼Œç›´åˆ°æ€»å’Œè¾¾åˆ° **p**ï¼Œåˆ°è¾¾**p**åå°†è¢«é€‰ä¸­çš„**token**ä»¬å°†è¢«ç”¨äºé‡‡æ ·

å¦‚æœåŒæ—¶æŒ‡å®šäº†`top_k`å’Œ`top_p`ï¼Œåˆ™ä¼šå…ˆä½¿ç”¨`top_k`è¿‡æ»¤å‰**k**ä¸ª**token**ï¼Œå†ä½¿ç”¨æ¦‚ç‡**p**è¿›è¡Œè¿‡æ»¤ï¼Œè‹¥ç´¯åŠ ä¸åˆ°**p**ï¼Œåˆ™å–å…¨éƒ¨å€™é€‰**token**è¿›è¡Œé‡‡æ ·

#### ä»æœ¬åœ°æ–‡ä»¶ä¸­åŠ è½½æ¨¡å‹

å°†`model`å‚æ•°æŒ‡å®šä¸ºæœ¬åœ°æ–‡ä»¶è·¯å¾„å³å¯ï¼Œä¾‹å¦‚ä¸€èˆ¬ä»**Hugggingface Hub**ä¸‹è½½çš„æ¨¡å‹ä½äºï¼š`~/.cache/huggingface/hub`ä¸‹ï¼Œå¯¹äºä¸Šè¿°ç¤ºä¾‹ä¸­çš„æ¨¡å‹ï¼Œå°†å…¶ä¸‹è½½åˆ°æœ¬åœ°åå¯ä»¥é€šè¿‡ï¼š

```python
generator = pipeline("text-generation", model="/home/u/.cache/huggingface/hub/models--unsloth--Qwen3-14B/snapshots/b8755c0b498d7b538068383748d6dc20397b4d1f")
```

è¿›è¡ŒåŠ è½½ï¼Œ**æ³¨æ„**è·¯å¾„ä¸€å®šè¦æŒ‡å®šåˆ°**snapshot**å†ä¸‹é¢ä¸€å±‚

***

## AutoTokenizerã€AutoModelForCausalLM

è¿™æ˜¯å¦ä¸€ç§åŠ è½½æ¨¡å‹çš„åŠæ³•ï¼Œå®ƒå°†**åˆ†è¯å™¨**å’Œ**æ¨¡å‹**åˆ†åˆ«åŠ è½½ï¼Œ`pipeline`ä»…ç”¨äºæ¨ç†ä½¿ç”¨ï¼Œä½¿ç”¨`AutoTokenizer`å’Œ`AutoModelForCausalLM`èƒ½ç”¨äºè®­ç»ƒå’Œæ¨ç†

é¦–å…ˆå¯¼å…¥äºŒè€…ï¼š

```python
from transfomers import AutoTokenizer, AutoModelForCausalLM
```

ç°ä»£**llm**å‘å¸ƒæ—¶å¾€å¾€å°†**åˆ†è¯å™¨**å’Œ**æ¨¡å‹**åˆå¹¶å‘å¸ƒï¼Œè¿™æ„å‘³ç€å¾€å¾€å¯ä»¥ä½¿ç”¨åŒä¸€ä¸ª`repo_id`æˆ–è€…**æœ¬åœ°è·¯å¾„**è¿›è¡ŒäºŒè€…çš„åŠ è½½ï¼Œå¦‚ï¼š

æ ¹æ®`repo_id`ä»**Huaggingface Hub**æ‹‰å–åˆ†è¯å™¨å’Œæ¨¡å‹ï¼ˆæœ‰ç¼“å­˜ä¼šè‡ªåŠ¨ä½¿ç”¨ï¼‰ï¼š

```python
repo_id = "unsloth/Qwen3-14B"

tokenizer = AutoTokenizer.from_pretrained(repo_id)
model = AutoModelForCausalLM.from_pretrained(repo_id)
```

ä»æœ¬åœ°è·¯å¾„åŠ è½½ï¼š

```python
local_path = "/home/u/.cache/huggingface/hub/models--unsloth--Qwen3-14B/snapshots/b8755c0b498d7b538068383748d6dc20397b4d1f"

tokenizer = AutoTokenizer.from_pretrained(local_path)
model = AutoModelForCausalLM.from_pretrained(local_path)
```

ä¼˜å…ˆçº§ä¸ºå…ˆæ£€æŸ¥ä¼ å…¥çš„å‚æ•°æ˜¯å¦ä¸ºæœ¬åœ°æ–‡ä»¶å¤¹ï¼Œå†æ£€æŸ¥**Hub**æ˜¯å¦æœ‰å¯¹åº”ä»“åº“

***

### å…³äºtokenizer

ä»å­—ç¬¦ä¸²åˆ°**token_id**åˆ—è¡¨çš„è¿‡ç¨‹å¾€å¾€éœ€è¦ç»å†ä¸‹è¿°è¿‡ç¨‹ï¼š

**Normalizer â†’ PreTokenizer â†’ Model (BPE merges) â†’ PostProcessor**

* **Normalizer**éƒ¨åˆ†ä¼šå¯¹å­—ç¬¦ä¸²åšä¸€äº›é¢„å¤„ç†ï¼Œå®ƒæ¥æ”¶åŸå§‹å­—ç¬¦ä¸²ï¼Œè¾“å‡ºé¢„å¤„ç†åçš„å­—ç¬¦ä¸²

* **Pretokenizer**æ¥æ”¶å¤„ç†åçš„å­—ç¬¦ä¸²ï¼Œä¾‹å¦‚`"ä»Šå¤©å¤©æ°”çœŸå¥½,I wanna go swimming"`ï¼ŒæŒ‰ç…§ä¸€å®šçš„è§„åˆ™å°†å­—ç¬¦ä¸²æ‹†ä¸ºè‹¥å¹²å—å­å­—ç¬¦ä¸²ï¼ˆä¾‹å¦‚æŒ‰ç…§ç©ºæ ¼åˆ†å‰²ï¼‰ï¼Œæ‹¿åˆ°ç±»ä¼¼äº` ['ä»Šå¤©', 'å¤©æ°”', 'çœŸ', 'å¥½', ',I', ' wanna', ' go', ' swimming']`è¿™æ ·çš„åˆ‡åˆ†ï¼Œæ¥ç€å®ƒä¼šåšä¸€ä¸ªæ˜ å°„ï¼Œå°†æ¯ä¸ªå­å­—ç¬¦ä¸²è½¬ä¸ºå­—èŠ‚æµï¼Œå†å°†æ¯ä¸ªå­—èŠ‚æµæŒ‰ç…§ä¸€ä¸ªå­—èŠ‚å¯¹åº”ä¸€ä¸ª**å¯æ‰“å°å­—ç¬¦**çš„æ˜ å°„å…³ç³»åšæ˜ å°„ï¼Œå¾—åˆ°`['Ã¤Â»Ä¬Ã¥Â¤Â©Ã¥Â¤Â©Ã¦Â°Ä¶Ã§Ä¾ÅÃ¥Â¥Â½', ',I', 'Ä wanna', 'Ä go', 'Ä swimming']`ä½œä¸ºè¾“å‡ºï¼Œè¿™è¢«ç§°ä¸º**pre-token**ï¼Œå…·ä½“å­—èŠ‚åˆ°**å¯æ‰“å°å­—ç¬¦**çš„æ˜ å°„å…³ç³»è§åæ–‡ï¼Œæ­¤å¤–å®ƒè¾“å‡ºæ—¶è¿˜ä¼šå¸¦ä¸Š**offset_mapping**ä½œä¸ºè®°å½•æ¯ä¸ªå­å­—ç¬¦ä¸²å¯¹åº”åŸå­—ç¬¦ä¸²ä¸­çš„ç´¢å¼•ä½ç½®çš„ä¿¡æ¯ï¼Œå®é™…è¾“å‡ºä¸ºï¼š

    `[('Ã¤Â»Ä¬Ã¥Â¤Â©Ã¥Â¤Â©Ã¦Â°Ä¶Ã§Ä¾ÅÃ¥Â¥Â½', (0, 6)), (',I', (6, 8)), ('Ä wanna', (8, 14)), ('Ä go', (14, 17)), ('Ä swimming', (17, 26))]`

    ä¸Šè¿°è¾“å‡ºä½¿ç”¨ä»£ç ï¼š`print(tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str("ä»Šå¤©å¤©æ°”çœŸå¥½,I wanna go swimming"))`å¾—åˆ°

* **Model(BPE merges)**æ˜¯å®é™…è´Ÿè´£åˆ†è¯çš„æ¨¡å‹éƒ¨åˆ†ï¼Œå®ƒæ‹¿åˆ°ä¸Šä¸€æ­¥è¾“å‡ºçš„**pre-tokens**å’Œ**offset_mapping**ï¼Œåœ¨æ¯ä¸ª**pre-token**å†…éƒ¨ä½¿ç”¨**BPE**ç®—æ³•è¿›è¡Œè¿›ä¸€æ­¥çš„æ‹†åˆ†ï¼ˆæˆ–è€…è¯´åˆå¹¶ï¼Œè¿™å–å†³äºçœ‹å¾…æ¯ä¸ª**pre-token**çš„æ–¹å¼ï¼‰ï¼Œæ‹¿åˆ°æœ€ç»ˆçš„**tokens**ï¼ˆè¯¥ç®—æ³•æ›´å…³å¿ƒé¢‘ç‡è€Œéè¯­æ„ç›¸å…³æ€§ï¼‰ï¼š`['Ã¤Â»Ä¬Ã¥Â¤Â©','Ã¥Â¤Â©Ã¦Â°Ä¶','Ã§Ä¾Å','Ã¥Â¥1â„2','Ã¯1â„4Ä®','I','Ä wanna','Ä go','Ä swimming']`

    ä½¿ç”¨ä»£ç ï¼š

    `print(tokenizer.tokenize("ä»Šå¤©å¤©æ°”çœŸå¥½,I wanna go swimming"))`

    æ‹¿åˆ°è¾“å‡ºçš„**tokens**ï¼š

    `['Ã¤Â»Ä¬Ã¥Â¤Â©', 'Ã¥Â¤Â©Ã¦Â°Ä¶', 'Ã§Ä¾Å', 'Ã¥Â¥Â½', ',I', 'Ä wanna', 'Ä go', 'Ä swimming']`

    æœ€åå®ƒä¼šå°†æ¯ä¸ª**token**å¯¹åº”åˆ°**token_id**ï¼Œå¾—åˆ°æœ€ç»ˆè¾“å‡ºï¼š

    `[100644, 104307, 88051, 52801, 37768, 32733, 728, 23380]`

    ä½¿ç”¨ä»£ç ï¼š

    `print(tokenizer("ä»Šå¤©å¤©æ°”çœŸå¥½,I wanna go swimming")["input_ids"])`å¯ä»¥æ‹¿åˆ°ä¸Šè¿°**token_ids**è¾“å‡º

    å¹¶ä¸”å®ƒä¹Ÿä¼šå¯¹**offset_mapping**è¿›è¡Œå¤„ç†ï¼Œä½¿ç”¨ä¸‹è¿°ä»£ç ï¼š

    `print(tokenizer("ä»Šå¤©å¤©æ°”çœŸå¥½,I wanna go swimming", return_attention_mask=False, return_offsets_mapping=True))`

    æ‹¿åˆ°è¾“å‡ºï¼š

    ```
    {
    	'input_ids': [100644, 104307, 88051, 52801, 37768, 32733, 728, 23380], 
    	'offset_mapping': [(0, 2), (2, 4), (4, 5), (5, 6), (6, 8), (8, 14), (14, 17), (17, 26)]
    }
    ```

* **PostProcessor**ä¼šåšä¸€äº›åå¤„ç†ï¼Œå®ƒæ˜¯å’Œ**Normalizer**ä¸€æ ·æ˜¯éå¿…è¦çš„

ä¸Šè¿°ä»‹ç»çš„æ˜¯åœ¨å­—èŠ‚å±‚é¢è¿›è¡Œçš„åˆ†è¯ï¼Œå®é™…ä¸Šä¹Ÿæœ‰å¾ˆå¤šå…¶ä»–æ–¹æ³•ï¼Œä½†**byte-level**æ˜¯ç›®å‰æœ€ä¸»æµçš„æ–¹æ³•ï¼Œå› ä¸ºåœ¨**BPE**é˜¶æ®µæŒ‰ç…§è¯è¡¨è¿›è¡Œåˆå¹¶ï¼Œä¸”æœªåˆå¹¶çš„å­—ç¬¦ä¿æŒä¸ºå•ä¸ªå­—ç¬¦ï¼Œè€Œå•ä¸ªå­—ç¬¦ä¹Ÿåœ¨è¯è¡¨ä¸­ï¼Œå®ƒå®Œå…¨çš„é¿å…äº†**oov**ã€‚

æ‹¿åˆ°åˆ†è¯å™¨å¯¹å­—ç¬¦ä¸²ç¼–ç åçš„**token_id**åˆ—è¡¨åï¼Œå°†å…¶å–‚ç»™çœŸæ­£è´Ÿè´£è¯­è¨€é€»è¾‘å¤„ç†çš„æ¨¡å‹ï¼Œæ‹¿åˆ°è¾“å‡ºçš„**token_id**å†è°ƒç”¨**tokenizer**è¿›è¡Œè§£ç ï¼Œè§£ç éƒ¨åˆ†ä¸ºå°†**token_id**è½¬ä¸º**token**ï¼Œå†æŒ‰ç…§ç‰¹å®šæ˜ å°„ï¼ˆä¸‹æ–‡æœ‰æ˜ å°„è¡¨ï¼‰æ˜ å°„å›å­—èŠ‚æµï¼Œå†å°†å­—èŠ‚æµè½¬ä¸ºäººç±»å¯è¯»çš„å­—ç¬¦ä¸²

#### æ­£å‘ç¼–ç 

##### ç›´æ¥è°ƒç”¨

é€šè¿‡ç›´æ¥è°ƒç”¨`tokenizer`å¯¹è±¡ï¼Œå³å¯å¾—åˆ°åˆ†è¯åçš„ç»“æœï¼Œå¦‚ï¼š

```python
prompt = "ä»Šå¤©å¤©æ°”çœŸå¥½"
encoded_input = tokenizer(prompt)
print(encoded_input)
```

è¿™å°†å¾—åˆ°ä¸€ä¸ª`transformers.tokenization_utils_base.BatchEncoding`å¯¹è±¡ï¼š

```shell
{'input_ids': [100644, 104307, 88051, 52801], 'attention_mask': [1, 1, 1, 1]}
```

å®ƒæ˜¯ä¸€ä¸ªç±»å­—å…¸å¯¹è±¡ï¼Œå…¶ä¸­`"input_ids"`å³ä¸ºåˆ†è¯åçš„ç»“æœï¼Œå®ƒä»£è¡¨æ¯ä¸ª**token**å¯¹åº”çš„**token_id**æ„æˆçš„åˆ—è¡¨ï¼Œå®é™…ä¸Šå®ƒä¹ŸçœŸæ˜¯ä¸€ä¸ª**list[int]**å¯¹è±¡ï¼›`"attention_mask"`æ˜¯ä¸€ä¸ªå’Œ`"input_ids"`ç­‰é•¿çš„**list[int]**ï¼Œä»£è¡¨æ³¨æ„åŠ›æ©ç ï¼Œè¿™éƒ¨åˆ†åç»­ä»‹ç»

ä¹Ÿå¯ä»¥æ‰¹é‡å¤„ç†ä¸€æ‰¹å­—ç¬¦ä¸²ï¼Œå¦‚ï¼š

```python
prompts = ["ä»Šå¤©å¤©æ°”çœŸå¥½", "æ³•å›½çš„é¦–éƒ½æ˜¯å·´é»"]
encoded_input = tokenizer(prompts)
print(encoded_input)
```

è¿™ä¹Ÿå°†å¾—åˆ°ä¸€ä¸ª`transformers.tokenization_utils_base.BatchEncoding`å¯¹è±¡ï¼Œé”®ä»ç„¶æ˜¯`"input_ids"`å’Œ`"attention_mask"`ï¼š

```shell
{
	'input_ids': [[100644, 104307, 88051, 52801], [104328, 9370, 59975, 100132, 106004]],
	'attention_mask': [[1, 1, 1, 1], [1, 1, 1, 1, 1]]
}
```

å”¯ä¸€ä¸åŒçš„æ˜¯å¤šåµŒå¥—äº†ä¸€å±‚åˆ—è¡¨

ä¸‹é¢ä»‹ç»ä¸€äº›ç›´æ¥è°ƒç”¨`tokenizer`çš„å‚æ•°ï¼š

`max_length`ï¼šç”¨äºæŒ‡å®šæ–‡æœ¬çš„æœ€å¤§**token**æ•°é‡

`truncation`ï¼šæŒ‡å®šå…¶ä¸º`True`åä¼šå°†ç¼–ç å**token**æ•°é‡å¤§äº`max_length`çš„**token**åºåˆ—è¿›è¡Œæˆªæ–­ï¼Œå–å‰`max_length`ä¸ª**token**

`padding`ï¼šæŒ‡å®šå¡«å……ï¼Œè®¾å®šä¸º`True`åä¼šå°†ç¼–ç å**token**æ•°é‡å°äºç‰¹å®šå€¼çš„**token**åºåˆ—è¿›è¡Œå¡«å……ï¼Œè¡¥å……é•¿åº¦åˆ°ç‰¹å®šå€¼ï¼Œè¿™é‡Œçš„ç‰¹å®šå€¼**æ‰€æœ‰tokenåºåˆ—é•¿åº¦ä¸­çš„æœ€å¤§å€¼**ï¼›è®¾å®šä¸º`"max_length"`åä¼šå°†é•¿åº¦å¡«å……ä¸º`"max_length"`ï¼›å¡«å……çš„**token**ä¸ºä¸€ä¸ªç‰¹æ®Šçš„ä¸“é—¨ç”¨äºå¡«å……çš„**token**ï¼Œå¯ä»¥é€šè¿‡`tokenizer.all_special_tokens`æŸ¥çœ‹æ‰€æœ‰ç‰¹æ®Š**token**ï¼Œå…¶ä¸­å°±åŒ…å«ç”¨äºå¡«å……çš„**token**å¯¹åº”çš„å­—ç¬¦ä¸²ï¼Œå®ƒå¾€å¾€æ˜¯ä¸€ä¸ªåŒ…å«**â€œpadâ€œ**çš„å­—ç¬¦ä¸²ï¼›é»˜è®¤åœ¨**token**åºåˆ—çš„å·¦è¾¹è¿›è¡Œå¡«å……ï¼Œå¯ä»¥é€šè¿‡æŒ‡å®š`tokenizer.padding_side = "right"`æŒ‡å®šåœ¨åºåˆ—å³è¾¹è¿›è¡Œå¡«å……ï¼Œé»˜è®¤ä½¿ç”¨å·¦å¡«å……æ˜¯å› ä¸ºæ–°ç”Ÿæˆçš„**token**å¯ä»¥ç›´æ¥åœ¨åé¢æ‹¼æ¥ï¼Œä½¿ç”¨å³å¡«å……è‹¥æ˜¯ç›´æ¥åœ¨åé¢æ‹¼æ¥å°†ä¼šå¯¼è‡´æ–°ç”Ÿæˆçš„**token**å’Œè¾“å…¥**token**åºåˆ—ä¸­é—´åŒ…å«è‹¥å¹²å¡«å……**token**

`return_offsets_mapping`ï¼šæŒ‡å®šæ­¤å‚æ•°ä¸º`True`åï¼Œå°†ä¼šåœ¨è¿”å›çš„å¯¹è±¡ä¸­å¢æ·»ä¸€ä¸ªé”®å€¼å¯¹ï¼Œé”®ä¸º`"offsets_mapping"`ï¼Œå€¼ä¸ºä¸€ä¸ª**list[tuple[int]]**æˆ–è€…**list[list[tuple[int]]]**ï¼ˆè¿™å–å†³äºä¼ è¿›å»çš„æ˜¯å­—ç¬¦ä¸²è¿˜æ˜¯å­—ç¬¦ä¸²åˆ—è¡¨ï¼‰ï¼Œä»£è¡¨åˆ†è¯åˆ‡åˆ†ä½ç½®å¯¹åº”çš„åŸå­—ç¬¦ä¸²ä¸­çš„ä½ç½®ç´¢å¼•ï¼Œå¦‚ï¼š

```shell
'offset_mapping': [(0, 2), (2, 4), (4, 5), (5, 6)]
'offset_mapping': [[(0, 2), (2, 4), (4, 5), (5, 6)], [(0, 2), (2, 3), (3, 4), (4, 6), (6, 8)]]
```

`return_tensors`ï¼šç”¨äºæŒ‡å®šè¿”å›çš„`"input_ids"`å’Œ`"attention_mask"`çš„ç±»å‹ï¼Œå®ƒä»¬é»˜è®¤æ˜¯åµŒå¥—åˆ—è¡¨ï¼Œå¯ä»¥é€šè¿‡æŒ‡å®šæ­¤å‚æ•°ä¸º`pt`ã€`tf`ã€`np`åˆ†åˆ«æŒ‡å®šè¿”å›çš„å¯¹è±¡ç±»å‹ä¸º`torch.tensor`ã€`tensorflow.tensor`ã€`numpy.ndarray`ï¼Œæ³¨æ„ï¼Œå¦‚æœæ²¡æœ‰æŒ‡å®š`truncation`æˆ–è€…`padding`ï¼Œåˆ™å¯èƒ½ä¼šå› ä¸ºå¥å­ç¼–ç åçš„**token**åºåˆ—çš„é•¿åº¦ä¸åŒè€Œæ— æ³•å°†åµŒå¥—åˆ—è¡¨è½¬ä¸ºå¼ é‡ï¼Œä¾‹å¦‚ï¼š

```python
prompts = ["ä»Šå¤©å¤©æ°”çœŸå¥½", "æ³•å›½çš„é¦–éƒ½æ˜¯å·´é»"]
encoded_input = tokenizer(prompts, return_tensors="pt")
print(encoded_input)
```

å°†å¾—åˆ°æŠ¥é”™ï¼š

```text
ValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).
```

æ‰€ä»¥åº”è¯¥é€šè¿‡æŒ‡å®šå…¶ä»–å‚æ•°ä¿è¯ç¼–ç å**token**åºåˆ—é•¿åº¦ä¸€è‡´ï¼Œæ‰èƒ½ä½¿`return_tensors`æ­£å¸¸ç”Ÿæ•ˆï¼Œå¦‚ï¼š

```python
prompts = ["ä»Šå¤©å¤©æ°”çœŸå¥½", "æ³•å›½çš„é¦–éƒ½æ˜¯å·´é»"]
encoded_input = tokenizer(prompts, padding=True, return_tensors="pt")
print(encoded_input)
```

è¿™å°†å¾—åˆ°ï¼š

```shell
{
	'input_ids': 
		tensor([[151654, 100644, 104307,  88051,  52801],
			    [104328,   9370,  59975, 100132, 106004]]),
	'attention_mask': 
		tensor([[0, 1, 1, 1, 1],
        	   [1, 1, 1, 1, 1]])
}
```

æ­¤å¤–å¦‚æœè¿”å›çš„é™¤äº†è¿™äºŒè€…è¿˜æœ‰å…¶ä»–é”®å€¼å¯¹ï¼ˆå¦‚å‰é¢çš„**offsets_mapping**ï¼‰ï¼Œå…¶ä¹Ÿä¼šè¢«ä¸€å¹¶è½¬æ¢ä¸ºç›¸åº”ç±»å‹

##### tokenizer.tokenizeæ–¹æ³•

æ­¤æ–¹æ³•ä¸ç›´æ¥è°ƒç”¨ä¸åŒï¼Œå®ƒå°†ç›´æ¥è¿”å›è¾“å…¥å­—ç¬¦ä¸²åˆ‡åˆ†åçš„åˆ—è¡¨ï¼Œå¦‚ï¼š

```python
prompt = "ä»Šå¤©å¤©æ°”çœŸå¥½"
print(tokenizer.tokenize(prompt))
```

è¿™å°†å¾—åˆ°ï¼š

```shell
['Ã¤Â»Ä¬Ã¥Â¤Â©', 'Ã¥Â¤Â©Ã¦Â°Ä¶', 'Ã§Ä¾Å', 'Ã¥Â¥Â½']
```

ä¸‹é¢æ˜¯å•å­—èŠ‚åˆ°**ä¾¿äºæ‰“å°**å­—ç¬¦çš„æ˜ å°„ï¼š

```python
def bytes_to_unicode():
    """
    ç”Ÿæˆå­—èŠ‚åˆ°Unicodeå­—ç¬¦çš„æ­£å‘æ˜ å°„è¡¨
    è¿”å›å­—å…¸ï¼š{byte_value: unicode_char}
    """
    # åŸå§‹ä¿ç•™çš„å­—èŠ‚èŒƒå›´
    bs = (
        list(range(ord("!"), ord("~") + 1)) +          # ASCIIå¯æ‰“å°å­—ç¬¦ï¼ˆ33-126ï¼‰
        list(range(ord("Â¡"), ord("Â¬") + 1)) +          # è¥¿ç­ç‰™è¯­ç‰¹æ®Šå­—ç¬¦ï¼ˆ161-172ï¼‰
        list(range(ord("Â®"), ord("Ã¿") + 1))            # å…¶ä»–æ‰©å±•å­—ç¬¦ï¼ˆ174-255ï¼‰
    )
    
    cs = bs.copy()  # åˆå§‹å­—ç¬¦åˆ—è¡¨
    n = 0
    
    # éå†æ‰€æœ‰å¯èƒ½çš„å­—èŠ‚ï¼ˆ0-255ï¼‰
    for b in range(2**8):
        if b not in bs:
            bs.append(b)
            cs.append(2**8 + n)  # è¶…å‡ºåŸå§‹èŒƒå›´çš„å­—èŠ‚æ˜ å°„åˆ°æ›´é«˜Unicodeç ä½
            n += 1
    
    # å°†ç ä½è½¬æ¢ä¸ºUnicodeå­—ç¬¦
    cs = [chr(code) for code in cs]
    
    return dict(zip(bs, cs))

print(sorted(bytes_to_unicode().items()))
```

```shell
[(0, 'Ä€'), (1, 'Ä'), (2, 'Ä‚'), (3, 'Äƒ'), (4, 'Ä„'), (5, 'Ä…'), (6, 'Ä†'), (7, 'Ä‡'), (8, 'Äˆ'), (9, 'Ä‰'), (10, 'ÄŠ'), (11, 'Ä‹'), (12, 'ÄŒ'), (13, 'Ä'), (14, 'Ä'), (15, 'Ä'), (16, 'Ä'), (17, 'Ä‘'), (18, 'Ä’'), (19, 'Ä“'), (20, 'Ä”'), (21, 'Ä•'), (22, 'Ä–'), (23, 'Ä—'), (24, 'Ä˜'), (25, 'Ä™'), (26, 'Äš'), (27, 'Ä›'), (28, 'Äœ'), (29, 'Ä'), (30, 'Ä'), (31, 'ÄŸ'), (32, 'Ä '), (33, '!'), (34, '"'), (35, '#'), (36, '$'), (37, '%'), (38, '&'), (39, "'"), (40, '('), (41, ')'), (42, '*'), (43, '+'), (44, ','), (45, '-'), (46, '.'), (47, '/'), (48, '0'), (49, '1'), (50, '2'), (51, '3'), (52, '4'), (53, '5'), (54, '6'), (55, '7'), (56, '8'), (57, '9'), (58, ':'), (59, ';'), (60, '<'), (61, '='), (62, '>'), (63, '?'), (64, '@'), (65, 'A'), (66, 'B'), (67, 'C'), (68, 'D'), (69, 'E'), (70, 'F'), (71, 'G'), (72, 'H'), (73, 'I'), (74, 'J'), (75, 'K'), (76, 'L'), (77, 'M'), (78, 'N'), (79, 'O'), (80, 'P'), (81, 'Q'), (82, 'R'), (83, 'S'), (84, 'T'), (85, 'U'), (86, 'V'), (87, 'W'), (88, 'X'), (89, 'Y'), (90, 'Z'), (91, '['), (92, '\\'), (93, ']'), (94, '^'), (95, '_'), (96, '`'), (97, 'a'), (98, 'b'), (99, 'c'), (100, 'd'), (101, 'e'), (102, 'f'), (103, 'g'), (104, 'h'), (105, 'i'), (106, 'j'), (107, 'k'), (108, 'l'), (109, 'm'), (110, 'n'), (111, 'o'), (112, 'p'), (113, 'q'), (114, 'r'), (115, 's'), (116, 't'), (117, 'u'), (118, 'v'), (119, 'w'), (120, 'x'), (121, 'y'), (122, 'z'), (123, '{'), (124, '|'), (125, '}'), (126, '~'), (127, 'Ä¡'), (128, 'Ä¢'), (129, 'Ä£'), (130, 'Ä¤'), (131, 'Ä¥'), (132, 'Ä¦'), (133, 'Ä§'), (134, 'Ä¨'), (135, 'Ä©'), (136, 'Äª'), (137, 'Ä«'), (138, 'Ä¬'), (139, 'Ä­'), (140, 'Ä®'), (141, 'Ä¯'), (142, 'Ä°'), (143, 'Ä±'), (144, 'Ä²'), (145, 'Ä³'), (146, 'Ä´'), (147, 'Äµ'), (148, 'Ä¶'), (149, 'Ä·'), (150, 'Ä¸'), (151, 'Ä¹'), (152, 'Äº'), (153, 'Ä»'), (154, 'Ä¼'), (155, 'Ä½'), (156, 'Ä¾'), (157, 'Ä¿'), (158, 'Å€'), (159, 'Å'), (160, 'Å‚'), (161, 'Â¡'), (162, 'Â¢'), (163, 'Â£'), (164, 'Â¤'), (165, 'Â¥'), (166, 'Â¦'), (167, 'Â§'), (168, 'Â¨'), (169, 'Â©'), (170, 'Âª'), (171, 'Â«'), (172, 'Â¬'), (173, 'Åƒ'), (174, 'Â®'), (175, 'Â¯'), (176, 'Â°'), (177, 'Â±'), (178, 'Â²'), (179, 'Â³'), (180, 'Â´'), (181, 'Âµ'), (182, 'Â¶'), (183, 'Â·'), (184, 'Â¸'), (185, 'Â¹'), (186, 'Âº'), (187, 'Â»'), (188, 'Â¼'), (189, 'Â½'), (190, 'Â¾'), (191, 'Â¿'), (192, 'Ã€'), (193, 'Ã'), (194, 'Ã‚'), (195, 'Ãƒ'), (196, 'Ã„'), (197, 'Ã…'), (198, 'Ã†'), (199, 'Ã‡'), (200, 'Ãˆ'), (201, 'Ã‰'), (202, 'ÃŠ'), (203, 'Ã‹'), (204, 'ÃŒ'), (205, 'Ã'), (206, 'Ã'), (207, 'Ã'), (208, 'Ã'), (209, 'Ã‘'), (210, 'Ã’'), (211, 'Ã“'), (212, 'Ã”'), (213, 'Ã•'), (214, 'Ã–'), (215, 'Ã—'), (216, 'Ã˜'), (217, 'Ã™'), (218, 'Ãš'), (219, 'Ã›'), (220, 'Ãœ'), (221, 'Ã'), (222, 'Ã'), (223, 'ÃŸ'), (224, 'Ã '), (225, 'Ã¡'), (226, 'Ã¢'), (227, 'Ã£'), (228, 'Ã¤'), (229, 'Ã¥'), (230, 'Ã¦'), (231, 'Ã§'), (232, 'Ã¨'), (233, 'Ã©'), (234, 'Ãª'), (235, 'Ã«'), (236, 'Ã¬'), (237, 'Ã­'), (238, 'Ã®'), (239, 'Ã¯'), (240, 'Ã°'), (241, 'Ã±'), (242, 'Ã²'), (243, 'Ã³'), (244, 'Ã´'), (245, 'Ãµ'), (246, 'Ã¶'), (247, 'Ã·'), (248, 'Ã¸'), (249, 'Ã¹'), (250, 'Ãº'), (251, 'Ã»'), (252, 'Ã¼'), (253, 'Ã½'), (254, 'Ã¾'), (255, 'Ã¿')]
```

è¿™ç§æ˜ å°„ä¸ä¼šå½±å“è‹±æ–‡å­—æ¯çš„æ‰“å°ï¼Œä½†ä¼šå½±å“éè‹±æ–‡å­—ç¬¦çš„æ‰“å°ï¼Œå¯é€šè¿‡ä¸‹è¿°ä»£ç æŸ¥çœ‹åŸå§‹æ–‡æœ¬ï¼š

```python
map_dict = {v: k for k, v in bytes_to_unicode().items()}
texts = ['Ã¤Â»Ä¬Ã¥Â¤Â©', 'Ã¥Â¤Â©Ã¦Â°Ä¶', 'Ã§Ä¾Å', 'Ã¥Â¥Â½']
original_texts = [bytes([map_dict[char] for char in text]).decode("utf-8") for text in texts]
print(original_texts)
```

```shell
['ä»Šå¤©', 'å¤©æ°”', 'çœŸ', 'å¥½']
```

å¦‚æœç»™`tokenize`æ–¹æ³•ä¼ çš„æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²åˆ—è¡¨ï¼Œå®ƒå°†ä¼šæŠŠåˆ—è¡¨ä¸­çš„æ‰€æœ‰å­—ç¬¦ä¸²æ‹¼æ¥æˆä¸€ä¸ªå­—ç¬¦ä¸²åå¤„ç†

##### tokenizer.convert_tokens_to_idsæ–¹æ³•

æ­¤æ–¹æ³•ç”¨äºå°†**token**åˆ—è¡¨è½¬æ¢ä¸º**id**åˆ—è¡¨ï¼Œå¦‚ï¼š

```python
tokens = ['Ã¤Â»Ä¬Ã¥Â¤Â©', 'Ã¥Â¤Â©Ã¦Â°Ä¶', 'Ã§Ä¾Å', 'Ã¥Â¥Â½']
print(tokenizer.convert_tokens_to_ids(tokens))
```

è¿™å°†å¾—åˆ°ï¼š

```shell
[100644, 104307, 88051, 52801]
```

å®ƒä¹Ÿèƒ½æ¥æ”¶å•ä¸ªå­—ç¬¦ä¸²ä½œä¸ºè¾“å…¥ï¼Œè¾“å‡ºå°†ä¸ºå•ä¸ª**int**ï¼Œå¦‚ï¼š

```python
print(tokenizer.convert_tokens_to_ids("<|vision_pad|>"))
```

ä¼šå¾—åˆ°ï¼š

```text
151654
```

#### åå‘è§£ç 

##### tokenizer.convert_ids_to_tokensæ–¹æ³•

æ­¤æ–¹æ³•æ¥æ”¶ä¸€ä¸ªä»£è¡¨**token_id**åºåˆ—çš„æ•´æ•°åˆ—è¡¨ï¼Œå°†å…¶è§£ç ä¸º**token**åˆ—è¡¨ï¼Œå¦‚ï¼š

```python
prompt = "ä»Šå¤©å¤©æ°”çœŸå¥½"
input_ids = tokenizer(prompt)["input_ids"]
print(tokenizer.convert_ids_to_tokens(input_ids))
```

å³å¯å¾—åˆ°ï¼š

```shell
['Ã¤Â»Ä¬Ã¥Â¤Â©', 'Ã¥Â¤Â©Ã¦Â°Ä¶', 'Ã§Ä¾Å', 'Ã¥Â¥Â½']
```

å®ƒä¹Ÿèƒ½æ¥æ”¶å•ä¸ª**int**ä½œä¸ºè¾“å…¥ï¼Œè¾“å‡ºå•ä¸ªå­—ç¬¦ä¸²ï¼Œå¦‚ï¼š

```python
print(tokenizer.convert_ids_to_tokens(151654))
```

ä¼šå¾—åˆ°ï¼š

```text
<|vision_pad|>
```

##### tokenizer.decodeæ–¹æ³•

æ­¤æ–¹æ³•æ¥æ”¶ä¸€ä¸ªä»£è¡¨**token_id**åºåˆ—çš„ã€ç±»å‹ä¸º**list[int]**ã€**np.ndarray**ã€**torch.tensor**ç­‰ï¼ˆåŸºæœ¬ä¸Šç›´æ¥æ‹¿**tokenizer**å¯¹å­—ç¬¦ä¸²ç›´æ¥ä½œç”¨è¿”å›çš„`input_ids`å°±è¡Œäº†ï¼‰çš„å¯¹è±¡ï¼Œå°†å…¶è§£ç ä¸º**token**åˆ—è¡¨å¹¶åˆå¹¶ä¸ºä¸€ä¸ª**èƒ½æ­£å¸¸æ˜¾ç¤ºéè‹±æ–‡å­—ç¬¦**å­—ç¬¦ä¸²ï¼Œå¦‚ï¼š

```python
prompt = "ä»Šå¤©å¤©æ°”çœŸå¥½"
input_ids = tokenizer(prompt)["input_ids"]
print(tokenizer.decode(input_ids))
```

å³å¯å¾—åˆ°`prompt`æœ¬èº«ï¼š

```shell
ä»Šå¤©å¤©æ°”çœŸå¥½
```

##### tokenizer.batch_decodeæ–¹æ³•

æ­¤æ–¹æ³•æ¥æ”¶ä¸€ä¸ªä»£è¡¨**token_ids**åºåˆ—çš„ã€**äºŒçº§åµŒå¥—çš„**ã€ç±»å‹ä¸º**list[int]**ã€**np.ndarray**ã€**torch.tensor**ç­‰ï¼ˆåŸºæœ¬ä¸Šç›´æ¥æ‹¿**tokenizer**å¯¹å­—ç¬¦ä¸²åˆ—è¡¨ç›´æ¥ä½œç”¨è¿”å›çš„`input_ids`å°±è¡Œäº†ï¼‰çš„å¯¹è±¡ï¼Œå°†å…¶è§£ç ä¸º**token**å¹¶å„è‡ªåˆå¹¶ä¸º**èƒ½æ­£å¸¸æ˜¾ç¤ºéè‹±æ–‡å­—ç¬¦**çš„å­—ç¬¦ä¸²ï¼Œå†æ”¾è¿›ä¸€ä¸ªåˆ—è¡¨ä¸­è¿”å›ï¼Œå¦‚ï¼š

```python
prompts = ["ä»Šå¤©å¤©æ°”çœŸå¥½", "æ³•å›½çš„é¦–éƒ½æ˜¯å·´é»"]
print(tokenizer.batch_decode(tokenizer(prompts)["input_ids"]))
```

å°†å¾—åˆ°ï¼š

```shell
['ä»Šå¤©å¤©æ°”çœŸå¥½', 'æ³•å›½çš„é¦–éƒ½æ˜¯å·´é»']
```

æ­¤å¤–å¦‚æœå¯¹ä¸€ä¸ªä¸€çº§åˆ—è¡¨ä½œç”¨ï¼Œå®ƒå°†å¾—åˆ°èƒ½æ­£å¸¸æ˜¾ç¤ºéè‹±æ–‡å­—ç¬¦çš„åˆ†è¯ç»“æœï¼š

```python
prompt = "ä»Šå¤©å¤©æ°”çœŸå¥½"
tokenizer.batch_decode(tokenizer(prompt)["input_ids"])
```

è¿™å°†å¾—åˆ°ï¼š

```shell
['ä»Šå¤©', 'å¤©æ°”', 'çœŸ', 'å¥½']
```

è¿™æ˜¯å› ä¸º`batch_decode`å†…éƒ¨å¯¹ä¼ è¿›å»çš„åˆ—è¡¨çš„æ¯ä¸€é¡¹åˆ†åˆ«åš`decode`åœ¨åˆå¹¶è¿›ä¸€ä¸ªåˆ—è¡¨è¿”å›çš„åŸå› ï¼Œè€Œå¯¹ä¸€ä¸ªä¸€çº§åˆ—è¡¨æ¥è¯´ï¼Œå…¶å†…éƒ¨çš„æ¯ä¸€ä¸ª`input_id`åš`decode`çš„ç»“æœå°±æ˜¯å…¶å¯¹åº”çš„**èƒ½æ­£å¸¸æ˜¾ç¤ºçš„éè‹±æ–‡å­—ç¬¦ä¸²**ï¼Œåœ¨æ”¾è¿›åˆ—è¡¨ä¸­è¿”å›å°±å¾—åˆ°äº†èƒ½æ­£å¸¸æ˜¾ç¤ºçš„åˆ†è¯ç»“æœï¼Œä¹Ÿå¯ä»¥è‡ªå·±ç”¨`decode`å¤„ç†ï¼š

```python
prompt = "ä»Šå¤©å¤©æ°”çœŸå¥½"
input_ids = tokenizer(prompt)["input_ids"]
print([tokenizer.decode(input_id) for input_id in input_ids])
```

è¿™ä¹Ÿå°†å¾—åˆ°**èƒ½æ­£å¸¸æ˜¾ç¤ºéè‹±æ–‡å­—ç¬¦**çš„åˆ†è¯ç»“æœï¼š

```shell
['ä»Šå¤©', 'å¤©æ°”', 'çœŸ', 'å¥½']
```

æ­¤å¤–ä¹Ÿå¯ä»¥é€šè¿‡å‰é¢ä»‹ç»è¿‡çš„`return_offsets_mapping`å‚æ•°æ‹¿åˆ°è¿”å›çš„**offset_mapping**ï¼Œç„¶åæ®å…¶ä»åŸå­—ç¬¦ä¸²ä¸­æ‰‹åŠ¨åˆ‡åˆ†å‡º**èƒ½æ­£å¸¸æ˜¾ç¤ºéè‹±æ–‡å­—ç¬¦**çš„åˆ†è¯ç»“æœ

å¯¹äºæ‰¹é‡çš„å­—ç¬¦ä¸²åˆ†è¯ï¼Œè¦æ­£ç¡®æ˜¾ç¤ºéè‹±æ–‡å­—ç¬¦ï¼Œé‡‡ç”¨ä¸Šè¿°ä¸‰ç§æ–¹æ³•è¿­ä»£å¤„ç†å³å¯

#### ä¿å­˜åˆ†è¯å™¨

ä½¿ç”¨`save_pretrained`æ–¹æ³•è¿›è¡Œä¿å­˜ï¼Œå‚æ•°å¡«è·¯å¾„å³å¯ï¼Œå¦‚ï¼š

```python
tokenizer.save_pretrained("tokenizer")
```

å³ä¼šå°†åˆ†è¯å™¨ä¿å­˜åˆ°å½“å‰è„šæœ¬æ‰€å±è·¯å¾„ä¸‹çš„`tokenizer`æ–‡ä»¶å¤¹ä¸‹

#### apply_chat_templateæ–¹æ³•

èŠå¤©æ¨¡æ¿æ˜¯å¯¹è¯ç±»è¯­è¨€æ¨¡å‹çš„åˆ†è¯å™¨å¸¦æœ‰çš„æ–¹æ³•ï¼Œç”¨äºå°†ä¸€ä¸ªåŒ…å«å¤šè½®å¯¹è¯çš„åˆ—è¡¨è½¬æ¢ä¸ºé¢„è®¾çš„ç‰¹å®šæ ¼å¼çš„å­—ç¬¦ä¸²ï¼Œ**è¿™ä¸ªæ¥å£åœ¨æ•°æ®å¤„ç†é˜¶æ®µéå¸¸å¸¸ç”¨**

é¦–å…ˆä½¿ç”¨`tokenizer.chat_template`æŸ¥çœ‹å…¶èŠå¤©æ¨¡æ¿ï¼Œè‹¥æ²¡æœ‰æ­¤å±æ€§é€šå¸¸è¡¨æ˜æ­¤åˆ†è¯å™¨ä¸æ˜¯å¯¹è¯ç±»è¯­è¨€æ¨¡å‹çš„åˆ†è¯å™¨ï¼Œæ— æ³•ä½¿ç”¨`apply_chat_template`æ–¹æ³•

`tokenizer.chat_template`çš„è¾“å‡ºæ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œå®ƒé‡‡ç”¨**Jinja2**æ¨¡æ¿è¯­è¨€å®šä¹‰äº†å¯¹è¯çš„æ ¼å¼åŒ–è§„åˆ™ï¼Œå½¢å¦‚ï¼š

```jinja2
{% for message in messages %}
    {% if message['role'] == 'user' %}
        {{'<|im_start|>user\n' + message['content'] + '<|im_end|>\n'}}
    {% elif message['role'] == 'assistant' %}
        {{'<|im_start|>assistant\n' + message['content'] + '<|im_end|>\n'}}
    {% else %}
        {{'<|im_start|>system\n' + message['content'] + '<|im_end|>\n'}}
    {% endif %}
{% endfor %}
{% if add_generation_prompt %}
    {{ '<|im_start|>assistant\n' }}
{% endif %}
```

**jinja2**æ˜¯ä¸€ç§æ¨¡æ¿è¯­è¨€ï¼Œä¹Ÿæ˜¯ä¸€ç§æ¨¡æ¿å¼•æ“ï¼Œå®ƒçš„è¯­æ³•éå¸¸ç›´è§‚ï¼Œè¯¦è§æœ¬æ–‡æ¡£åŒçº§ç›®å½•ä¸‹çš„å¦ä¸€ç¯‡æ–‡æ¡£ï¼Œè¿™é‡Œç”¨äºå®šä¹‰èŠå¤©æ¨¡æ¿çš„æ ¼å¼

ä½¿ç”¨`apply_chat_template`æ–¹æ³•ï¼Œå®ƒæ¥æ”¶ä¸€ä¸ª**list[dict[str, str]]**ï¼Œå°†å…¶æŒ‰ç…§**chat_template**ä¸­å®šä¹‰çš„æ¨¡æ¿è¿›è¡Œè½¬ä¸ºå•ä¸ªå­—ç¬¦ä¸²è¾“å‡ºã€‚å®ƒä¹Ÿå¯ä»¥æ¥æ”¶ä¸€ä¸ª**list[list[dict[str, str]]]**ï¼Œè¿”å›å¯¹å…¶ä¸­æ¯ä¸ª**list[dict[str, str]]**åˆ†åˆ«å¤„ç†åçš„å­—ç¬¦ä¸²æ„æˆçš„åˆ—è¡¨ï¼Œå¦‚ï¼š

```python
conversations = [{'role': 'user', 'content': 'ä½ å¥½'}, {'role': 'assistant', 'content': 'ä½ å¥½ï¼Œæœ‰ä»€ä¹ˆå¯ä»¥å¸®æ‚¨çš„ï¼Ÿ'}]
formatted_string = tokenizer.apply_chat_template(conversations, tokenize=False)
print(formatted_string)
```

è¿™å°†å¾—åˆ°ä¸€ä¸ªå­—ç¬¦ä¸²ï¼š

```text
<|im_start|>user
ä½ å¥½<|im_end|>
<|im_start|>assistant
<think>

</think>

ä½ å¥½ï¼Œæœ‰ä»€ä¹ˆå¯ä»¥å¸®æ‚¨çš„ï¼Ÿ<|im_end|>

```

è¿™å°±æ˜¯æŒ‰ç…§èŠå¤©æ¨¡æ¿æ ¼å¼åŒ–åçš„å­—ç¬¦ä¸²ï¼Œç¬¬ä¸€ä¸ªå‚æ•°ä¸€å®šè¦æŒ‡å®šä¸ºè¦è¾“å…¥çš„åˆ—è¡¨ï¼Œæˆ–è€…ç”¨`conversation`å‚æ•°åæŒ‡å®š

`tokenize`å‚æ•°é»˜è®¤ä¸º`True`ï¼Œå½“å…¶ä¸º`True`æ—¶ï¼Œå…¶è¾“å‡ºå°†ä¸ºä¸€ä¸ª**list[int]**ï¼š

```text
[151644, 872, 198, 108386, 151645, 198, 151644, 77091, 198, 151667, 271, 151668, 271, 108386, 3837, 104139, 73670, 99663, 101214, 11319, 151645, 198]
```

å®ƒå®é™…ä¸Šå¯¹åº”çš„æ˜¯ä¸Šè¿°å­—ç¬¦ä¸²è¢«**tokenizer**ä½œç”¨åçš„ç»“æœï¼Œå³ï¼š

`tokenizer(tokenizer.apply_chat_template(conversations, tokenize=False))["input_ids"]`

é€šè¿‡è§£ç å¯ä»¥æŸ¥çœ‹å…¶å¯¹åº”åˆ†è¯ç»“æœï¼š

```python
print(tokenizer.batch_decode(tokenizer.apply_chat_template(conversations)))
```

```text
['<|im_start|>', 'user', '\n', 'ä½ å¥½', '<|im_end|>', '\n', '<|im_start|>', 'assistant', '\n', '<think>', '\n\n', '</think>', '\n\n', 'ä½ å¥½', 'ï¼Œ', 'æœ‰ä»€ä¹ˆ', 'å¯ä»¥', 'å¸®', 'æ‚¨çš„', 'ï¼Ÿ', '<|im_end|>', '\n']
```

`return_tensors`ï¼šè¿™æ˜¯`apply_chat_template`çš„ä¸€ä¸ªé‡è¦å‚æ•°ï¼Œç”¨äºæŒ‡å®šè¿”å›çš„**token_ids**çš„ç±»å‹ï¼Œç±»ä¼¼äºå‰é¢ä»‹ç»è¿‡çš„ç›´æ¥è°ƒç”¨`tokenizer`çš„åŒåå‚æ•°ï¼Œä¸€èˆ¬è¦å–‚ç»™æ¨¡å‹ï¼ŒæŒ‡å®šå…¶ä¸º`"pt"`å³å¯ã€‚æŒ‡å®šè¯¥å‚æ•°ä¸º`"pt"`ä¹‹åï¼Œå³ä½¿`conversations`æ˜¯ä¸€ä¸ª**list[dict[str, str]]**è€Œé**list[list[dict[str, str]]]**ï¼Œå…¶ä¹Ÿå°†è¿”å›ä¸€ä¸ªäºŒç»´å¼ é‡ï¼ˆ**shape**ä¸º`torch.Size([1, n])`ï¼‰ï¼Œè¿™æ˜¯å› ä¸ºåç»­çš„`model.generate`æ‹’ç»å¤„ç†ä¸€ç»´å¼ é‡ï¼Œéœ€è¦åœ¨å¤–å±‚**unsqueeze**ä¸€å±‚**batch_size**ç»´åº¦

**é™¤äº†ä¸Šè¿°çš„å‚æ•°ï¼Œæ­¤æ–¹æ³•è¿˜æœ‰å¦å¤–ä¸€ç§å‚æ•°ï¼Œå®ƒä»¬ä¼šè¢«ä¼ ç»™chat_templateç”¨äºå…·ä½“çš„æ¸²æŸ“ï¼Œè¿™æ„å‘³ç€ä¸åŒçš„chat_templateå°†ä¼šèƒ½å¤„ç†ä¸åŒçš„å‚æ•°ï¼Œè¦ä¾æ®å…¶å†…å®¹å…·ä½“è€Œå®š**ï¼Œä¸¾ä¾‹æ¥è¯´ï¼Œ**unsloth/Qwen3-14B**çš„**chat_template**ä¸­æœ‰è¿™ä¹ˆä¸€æ®µå†…å®¹ï¼š

```jinja2
{%- if add_generation_prompt %}
    {{- '<|im_start|>assistant\n' }}
    {%- if enable_thinking is defined and enable_thinking is false %}
        {{- '<think>\n\n</think>\n\n' }}
    {%- endif %}
{%- endif %}
```

é‡Œé¢ç”¨åˆ°äº†`add_generation_prompt`å’Œ`enable_thinking`ä¸¤ä¸ªå‚æ•°ï¼Œå¹¶æŒ‡æ˜äº†å…·ä½“çš„ä½¿ç”¨é€»è¾‘ï¼Œè¿™æ„å‘³ç€ä½ å°†å¯ä»¥ç»™`apply_chat_template`æ–¹æ³•ä¼ é€’è¿™ä¸¤ä¸ªå‚æ•°ï¼š

`add_generation_prompt`ï¼šè¯¥å‚æ•°çš„é»˜è®¤å€¼ä¸º**False**ï¼ˆå®ƒä¸æ˜¯å¯å˜å…³é”®å­—å‚æ•°ï¼Œæ‰€ä»¥æœ‰é»˜è®¤å€¼ï¼Œä½†æ˜¯å®ƒä¹Ÿèƒ½åœ¨**chat_template**è¢«ç”¨åˆ°ï¼‰ï¼Œæ ¹æ®ä¸Šè¿°**chat_template**ï¼ŒæŒ‡å®šå…¶ä¸º**True**ä¹‹åï¼Œå°†ä¼šåœ¨è¿”å›çš„**token_ids**åé¢æ·»åŠ ä¸€äº›**tokens_ids**ï¼Œå®ƒä»¬å¯¹åº”ç€æ‰€è°“çš„**â€generation_promptâ€œ**ï¼Œå…·ä½“å†…å®¹å–å†³äºä¸Šè¿°**chat_template**ï¼Œä¸¾ä¾‹æ¥è¯´ï¼š

```python
token_ids_with_generation_prompt = tokenizer.apply_chat_template(conversations, add_generation_prompt=True)
print(tokenizer.batch_decode(token_ids_with_generation_prompt))
```

è¿™å°†å¾—åˆ°ï¼š

```text
['<|im_start|>', 'user', '\n', 'ä½ å¥½', '<|im_end|>', '\n', '<|im_start|>', 'assistant', '\n', '<think>', '\n\n', '</think>', '\n\n', 'ä½ å¥½', 'ï¼Œ', 'æœ‰ä»€ä¹ˆ', 'å¯ä»¥', 'å¸®', 'æ‚¨çš„', 'ï¼Ÿ', '<|im_end|>', '\n', '<|im_start|>', 'assistant', '\n']
```

å¯ä»¥çœ‹åˆ°æœ«å°¾å¤šäº†ä¸€äº›**token**ï¼š`'<|im_start|>', 'assistant', '\n'`ï¼Œè¿™å°±æ˜¯æ‰€è°“çš„**generation_prompt**ï¼Œå®ƒç”¨äºæç¤ºæ¨¡å‹è¯¥ä½œä¸ºåŠ©æ‰‹å›ç­”å‰æ–‡çš„é—®é¢˜ï¼Œè€Œä¸æ˜¯æ™®é€šçš„ç»­å†™ã€‚

`enable_thinking`ï¼šæ ¹æ®ä¸Šè¿°**chat_template**ï¼Œè¯¥å‚æ•°å¿…é¡»åœ¨`add_generation_prompt`æŒ‡å®šä¸º**True**ä¸”å…¶è‡ªèº«æŒ‡å®šä¸º**False**æ—¶æ‰ä¼šäº§ç”Ÿå½±å“ï¼Œå®ƒå°†åœ¨åé¢åŠ ä¸Šå­—ç¬¦ä¸²`"<think>\n\n</think>\n\n"`å¯¹åº”çš„**token_ids**ï¼Œè¿™å°†ä½¿å¾—æ¨¡å‹æ¥ä¸‹æ¥å¤„ç†çš„æ—¶å€™è·³è¿‡æ€è€ƒé˜¶æ®µï¼Œè¿™æ˜¯ä¸€ç§ç›´è§‚çš„æ–¹æ³•ï¼š

```python
token_ids_with_generation_prompt_and_empty_thinking = tokenizer.apply_chat_template(conversations, add_generation_prompt=True)
print(tokenizer.batch_decode(token_ids_with_generation_prompt))
```

```text
['<|im_start|>', 'user', '\n', 'ä½ å¥½', '<|im_end|>', '\n', '<|im_start|>', 'assistant', '\n', '<think>', '\n\n', '</think>', '\n\n', 'ä½ å¥½', 'ï¼Œ', 'æœ‰ä»€ä¹ˆ', 'å¯ä»¥', 'å¸®', 'æ‚¨çš„', 'ï¼Ÿ', '<|im_end|>', '\n', '<|im_start|>', 'assistant', '\n', '<think>', '\n\n', '</think>', '\n\n']
```

è¿™äº›**chat_template**å†…éƒ¨ä¼šç”¨åˆ°çš„å‚æ•°å’Œ`tokenize`ã€`return_tensors`ä½œç”¨æ˜¯äº’ç›¸ç‹¬ç«‹çš„

***

### å…³äºmodel

è¿™æ˜¯çœŸæ­£è´Ÿè´£å¤„ç†è¯­è¨€é€»è¾‘çš„éƒ¨åˆ†ï¼Œåœ¨**tokenizer**å¯¹å­—ç¬¦ä¸²å¤„ç†åè¦å°†**token_ids**äº¤ç”±æ¨¡å‹è¿›è¡Œç»­å†™ï¼Œæ¨¡å‹è¾“å‡º**token_id**å¹¶è¿”å›ç»™**tokenizer**è¿›è¡Œè§£ç 

å‰é¢ä»‹ç»äº†ä½¿ç”¨`AutoModelForCausalLM.from_pretrained`æ–¹æ³•åŠ è½½æ¨¡å‹çš„æ–¹æ³•ï¼Œä¸‹é¢ä»‹ç»è¯¥æ–¹æ³•çš„ä¸€äº›å‚æ•°ï¼š

`device_map`ï¼šæ­¤å‚æ•°ç”¨äºæŒ‡å®šå°†æ¨¡å‹çš„å„éƒ¨åˆ†åˆ†åˆ«åŠ è½½åˆ°å“ªä¸ªè®¾å¤‡ä¸Šï¼Œä¸æ˜¾å¼æŒ‡æ˜è¯¥å‚æ•°ï¼Œæ¨¡å‹å°†ä¼šè¢«å…¨éƒ¨åŠ è½½åˆ°å†…å­˜ä¸­ï¼Œå…¶**device**ä¸º**cpu**ã€‚æŒ‡å®šè¯¥å‚æ•°ä¸º**â€auto"**ï¼Œåº“å°†è‡ªåŠ¨æ£€æŸ¥å¯ç”¨ç¡¬ä»¶èµ„æºï¼Œä¼˜å…ˆåŠ è½½åˆ°æ˜¾å­˜ä¸­å¹¶æŒ‡å®š**device**ä¸º**cuda**ï¼Œåç»­æ˜¾å­˜ä¸è¶³å°†ä¼šæŠŠå…¶ä½™éƒ¨åˆ†**offload**åˆ°å†…å­˜ä¸­

`dtype`ï¼šæ­¤å‚æ•°ç”¨äºæŒ‡å®šæ¨¡å‹çš„å‚æ•°çš„æ•°æ®ç±»å‹ï¼Œ**é»˜è®¤ä¸º`torch.float32`**ã€‚æŒ‡å®šå…¶ä¸º**auto**åå°†ä¼šæ ¹æ®æ¨¡å‹é…ç½®æ–‡ä»¶ä¸­æŒ‡å®šçš„æ•°æ®ç±»å‹è¿›è¡ŒåŠ è½½ã€‚åŠ è½½åå¯ä½¿ç”¨`print(model.dtype)`æŸ¥çœ‹å…¶å€¼

å¦‚æœä¸è¿›è¡Œé‡åŒ–åŠ è½½ï¼Œå»ºè®®ä¿æŒ`dtype`ä¸º**auto**

***

#### generateæ–¹æ³•

æ­¤æ–¹æ³•ç”¨äºç”Ÿæˆï¼Œç±»ä¼¼äºæœ¬æ–‡æ¡£å¼€å¤´ä»‹ç»è¿‡çš„ç›´æ¥è°ƒç”¨`Pipeline`ï¼Œå®ƒæ¥æ”¶çš„é¦–ä¸ªå‚æ•°ä¸º`"iputs"`ï¼Œæ˜¯ä¸€ä¸ªäºŒç»´çš„å¼ é‡ï¼Œè§†ç¬¬ä¸€ä¸ªç»´åº¦ä¸º**batch_size**ï¼Œè§†ç¬¬äºŒä¸ªç»´åº¦ä¸ºå…·ä½“çš„**token_id**åºåˆ—ï¼Œå…¶è¾“å‡ºä¸€ä¸ªäºŒç»´å¼ é‡ï¼Œç¬¬ä¸€ä¸ªç»´åº¦ä¿æŒä¸å˜ï¼Œåœ¨ç¬¬äºŒä¸ªç»´åº¦ä¸Šè¿›è¡Œæ‹“å±•ï¼Œè¿”å›ç»­å†™åçš„**token_id**åºåˆ—ï¼Œå¦‚ï¼š

```python
model_inputs = tokenizer(["A list of colors: red, blue"], return_tensors="pt").to(model.device)
generated_ids = model.generate(model_inputs["input_ids"], max_new_tokens=50)
print(tokenizer.batch_decode(generated_ids))
```

è¿™å°†å¾—åˆ°ï¼š

```text
["A list of colors: red, blue, green, yellow, black, white, orange, purple, pink, brown, gray, teal, magenta, cyan, indigo, violet, gold, silver.  How many colors are in the list? Let's count them one by"]
```

å¦‚æœä¸æŒ‡å®š`max_new_tokens`è¿›è¡Œç»­å†™é•¿åº¦çš„é™åˆ¶ï¼Œå…¶å°†ä½¿ç”¨æ¨¡å‹é…ç½®æ–‡ä»¶`generation_config.json`ä¸­æŒ‡å®šçš„`max_length`è¿›è¡Œé™å®šï¼Œè€Œ`max_length`å¯èƒ½éå¸¸å¤§ï¼Œæ‰€ä»¥ä¸€èˆ¬éœ€è¦æ‰‹åŠ¨æŒ‡å®šï¼Œå¦å¤–ä¹Ÿå¯ä»¥é€šè¿‡å‚æ•°`max_length`è¿›è¡Œé™å®šã€‚åœ¨åˆ°è¾¾ä¸Šé™ä¹‹å‰ï¼Œå®ƒå¯èƒ½å› ä¸ºç”Ÿæˆäº†ç»ˆæ­¢**token**è€Œåœæ­¢ç»­å†™

æ­¤å¤–å®ƒä¹Ÿæ”¯æŒ`top_p`ã€`top_k`ã€`temperature`ä¸‰ä¸ªå‚æ•°ï¼Œå…·ä½“ä½œç”¨å·²ç»åœ¨æœ¬æ–‡æ¡£å¼€å¤´çš„**pipeline**éƒ¨åˆ†ä»‹ç»è¿‡

æ­¤å¤–ï¼Œå®ƒè¿˜æ”¯æŒ`attention_mask`å‚æ•°ï¼Œå› æ­¤ä½ å¯ä»¥é‡‡ç”¨è¿™ç§å†™æ³•ï¼š

```python
model_inputs = tokenizer(["A list of colors: red, blue"], return_tensors="pt").to(model.device)
generated_ids = model.generate(**model_inputs, max_new_tokens=50)
print(tokenizer.batch_decode(generated_ids))
```

æ³¨æ„åŠ›æ©ç åœ¨ä¼ é€’å•ä¸ªåºåˆ—æ—¶å…¨ä¸º**1**ï¼Œåœ¨ä¼ é€’æ‰¹é‡æ•°æ®æ—¶ï¼Œå› ä¸ºå„åºåˆ—é•¿çŸ­ä¸ä¸€ï¼Œè¦è¿›è¡Œ**padding**ï¼ˆåœ¨å‰æ–‡ç›´æ¥è°ƒç”¨`tokenizer`éƒ¨åˆ†çš„`padding`å‚æ•°ä»‹ç»è¿‡ï¼‰æ­¤æ—¶çš„æ©ç å¯èƒ½å½¢å¦‚ï¼š

```text
tensor([[1, 1, 1, 1, 1, 1, 1, 1],
        [0, 0, 0, 0, 1, 1, 1, 1]], device='cuda:0')
```

**æ­¤æ—¶åº”è¯¥ä¼ é€’æ©ç è¿›å»**

`do_sample`ï¼šé»˜è®¤ä¸º**True**ï¼Œä¸º**True**æ—¶æ­£å¸¸é‡‡æ ·ï¼Œä¸º**False**æ—¶æ¯ä¸€æ­¥éƒ½ä¼šé€‰å–æ¦‚ç‡æœ€é«˜çš„**token**

`num_return_sequences`ï¼šé»˜è®¤ä¸º**1**ï¼Œæ­¤å‚æ•°ç”¨äºæŒ‡å®šæ¯ä¸ªæ ·æœ¬çš„ç»­å†™æ•°é‡ï¼Œä¸¾ä¾‹æ¥è¯´ï¼Œå…¶ä¸ºä¸€çš„æ—¶å€™è¾“å‡ºçš„**generated_ids**çš„**shape**è‹¥ä¸º`torch.Size([2, 58])`ï¼Œåˆ™å…¶ä¸ºä¸‰çš„æ—¶å€™ä¸º`torch.Size([6, 58])`ï¼Œå…¶ä¸­æ²¿ç€**batch_size**ç»´åº¦ï¼Œå‰ä¸‰ä¸ªå¯¹åº”ç¬¬ä¸€ä¸ªåºåˆ—çš„ç”Ÿæˆç»“æœ

`streamer`ï¼šæ­¤å‚æ•°ç”¨äºæµå¼æ‰“å°ï¼Œæ¥æ”¶çš„å¯¹è±¡ä¸ºä¸€ä¸ª`TextStreamer`å¯¹è±¡ï¼Œä½¿ç”¨`from transformers import TextStreamer`è¿›è¡Œå¯¼å…¥ï¼Œå°†å…¶ä¼ å…¥`generate`æ–¹æ³•å³å¯è¿›è¡Œæµå¼ç”Ÿæˆï¼š

```python
from transformers import TextStreamer

message = [
    {"role": "user", "content": "ä½ å¥½"}
]
text = tokenizer.apply_chat_template(
    message,
    tokenize=False,
    add_generation_prompt=True,  # Must add for generation
    enable_thinking=False,  # Disable thinking
)
outputs = model.generate(
    **tokenizer(text, return_tensors="pt").to("cuda"),
    max_new_tokens=256,  # Increase for longer outputs!
    temperature=0.7, top_p=0.8, top_k=20,  # For non thinking
    streamer=TextStreamer(tokenizer)
)
```

å®ƒå°†è‡ªåŠ¨æµå¼æ‰“å°ç”Ÿæˆç»“æœï¼š

```text
<|im_start|>user
ä½ å¥½<|im_end|>
<|im_start|>assistant
<think>

</think>

ä½ å¥½ï¼å¾ˆé«˜å…´è§åˆ°ä½ ï¼ğŸ˜Š æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®ä½ çš„å—ï¼Ÿæ— è®ºæ˜¯èŠå¤©ã€è§£å†³é—®é¢˜ï¼Œè¿˜æ˜¯å•çº¯æƒ³æ‰¾ä¸ªä¼´å„¿ï¼Œæˆ‘éƒ½åœ¨è¿™å„¿ï¼<|im_end|>
```

å¦‚æœä¸éœ€è¦æ‰“å°å‰å¯¼æç¤ºè¯ï¼Œå¯ä»¥æŒ‡å®š`TextStreamer`çš„`skip_prompt`ä¸º**True**ï¼Œå³ï¼š
```python
outputs = model.generate(
    **tokenizer(text, return_tensors="pt").to("cuda"),
    max_new_tokens=256,  # Increase for longer outputs!
    temperature=0.7, top_p=0.8, top_k=20,  # For non thinking
    streamer=TextStreamer(tokenizer, skip_prompt=True)
)
```

è¿™å°†å•çº¯æ‰“å°æ–°ç”Ÿæˆçš„å†…å®¹ï¼š
```text
ä½ å¥½ï¼å¾ˆé«˜å…´è§åˆ°ä½ ï¼ğŸ˜Š æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®ä½ çš„å—ï¼Ÿæ— è®ºæ˜¯èŠå¤©ã€è§£å†³é—®é¢˜ï¼Œè¿˜æ˜¯å•çº¯æƒ³æ‰¾ä¸ªä¼´å„¿ï¼Œæˆ‘éƒ½åœ¨è¿™å„¿ï¼âœ¨<|im_end|>
```

éœ€è¦æ³¨æ„çš„æ˜¯å› ä¸º`<|im_end|>`æ˜¯ç»ˆæ­¢**token**ï¼Œæ‰€ä»¥ç”Ÿæˆåˆ°æ­¤ä¸­æ­¢äº†ï¼Œå¦‚æœè¦å¿½ç•¥ç»“å°¾çš„`<|im_end|>`ï¼Œå¯ä»¥æŒ‡å®šå‚æ•°`skip_special_tokens`ä¸º**True**ï¼Œå®ƒå°†å¿½ç•¥ä¸€åˆ‡ç‰¹æ®Š**token**çš„æ‰“å°ï¼š

```python
outputs = model.generate(
    **tokenizer(text, return_tensors="pt").to("cuda"),
    max_new_tokens=256,  # Increase for longer outputs!
    temperature=0.7, top_p=0.8, top_k=20,  # For non thinking
    streamer=TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)
)
```

```text
ä½ å¥½ï¼å¾ˆé«˜å…´è§åˆ°ä½ ã€‚æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®ä½ çš„å—ï¼ŸğŸ˜Š
```

æœ€ç»ˆæ‹¿åˆ°çš„è¿”å›å€¼`outputs`æ˜¯ä¸€ä¸ªäºŒç»´å¼ é‡ï¼Œä½¿ç”¨`batch_deocde`å³å¯è§£ç ï¼š

```python
print(tokenizer.batch_decode(outputs))
```

è¿™å°†å¾—åˆ°ï¼š

```text
['<|im_start|>user\nä½ å¥½<|im_end|>\n<|im_start|>assistant\n<think>\n\n</think>\n\nä½ å¥½ï¼å¾ˆé«˜å…´è§åˆ°ä½ ï¼ğŸ˜Š æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®ä½ çš„å—ï¼Ÿæ— è®ºæ˜¯èŠå¤©ã€è§£å†³é—®é¢˜ï¼Œè¿˜æ˜¯å•çº¯æƒ³æ‰¾ä¸ªä¼´å„¿èŠèŠï¼Œæˆ‘éƒ½å¾ˆä¹æ„ï¼<|im_end|>']
```

å¦‚æœè¦å®æ—¶æ‹¿åˆ°æ–°ç”Ÿæˆçš„**token**è€Œéå•çº¯æ‰“å°ï¼Œå¯ä»¥ä½¿ç”¨`TextIteratorStreamer`æ¥æ›¿ä»£`TextStreamer`ï¼Œå¦‚ï¼š

```python
from transformers import TextIteratorStreamer

streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)
outputs = model.generate(
    **tokenizer(text, return_tensors="pt").to("cuda"),
    max_new_tokens=256,  # Increase for longer outputs!
    temperature=0.7, top_p=0.8, top_k=20,  # For non thinking
    streamer=streamer
)
```

å®ƒæœ€ç»ˆæ‹¿åˆ°çš„**outputs**å’Œä½¿ç”¨`TextSteamer`ä¸€æ ·ï¼Œä¸åŒçš„æ˜¯å®ƒä¸ä¼šè¿›è¡Œæ‰“å°ï¼Œå¹¶ä¸”`streamer`æ˜¯ä¸€ä¸ªç±»ä¼¼äºè¿­ä»£å™¨çš„å¯¹è±¡ï¼Œå¯ä»¥é€šè¿‡éå†å®ƒæ¥æ‹¿åˆ°æ–°ç”Ÿæˆçš„**token**ï¼ˆéœ€è¦äº‹å…ˆåœ¨åå°çº¿ç¨‹ä¸­å¼€å¯`generate`æ–¹æ³•ï¼Œè¿™æ ·å®ƒæ‰ä¼šåœ¨åå°ä¸æ–­æŠŠæ–°ç”Ÿæˆçš„**token**æ”¾è¿›è¿­ä»£å™¨å†…ï¼‰ï¼š

```python
from transformers import TextIteratorStreamer
from threading import Thread

streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)
parameters = dict(
    **tokenizer(text, return_tensors="pt").to("cuda"),
    max_new_tokens=256,  # Increase for longer outputs!
    temperature=0.7, top_p=0.8, top_k=20,  # For non thinking
    streamer=streamer
)
background_thread = Thread(target=model.generate, kwargs=parameters)
background_thread.start()

for new_token in streamer:
	print("###" + new_token + "$$$")
```

è¿™å°†å¾—åˆ°ï¼š

```text
###ä½ å¥½$$$
###$$$
###ï¼å¾ˆé«˜å…´$$$
###è§åˆ°$$$
###ä½ $$$
###$$$
###$$$
###ï¼ğŸ˜Š $$$
###æœ‰ä»€ä¹ˆ$$$
###æˆ‘å¯ä»¥$$$
###å¸®$$$
###ä½ çš„$$$
###å—$$$
###$$$
###ï¼Ÿæ— è®ºæ˜¯$$$
###èŠå¤©$$$
###$$$
###ã€è§£å†³é—®é¢˜$$$
###$$$
###ï¼Œè¿˜æ˜¯$$$
###å•çº¯$$$
###æƒ³$$$
###æ‰¾ä¸ª$$$
###ä¼´$$$
###å„¿$$$
###èŠèŠ$$$
###$$$
###ï¼Œæˆ‘$$$
###éƒ½åœ¨$$$
###è¿™å„¿$$$
###$$$
###$$$
###$$$
###ï¼âœ¨$$$
```

***

#### ä¿å­˜æ¨¡å‹åˆ°æœ¬åœ°

ä½¿ç”¨`save_pretrained`æ¥å£è¿›è¡Œå·²åŠ è½½æ¨¡å‹çš„ä¿å­˜ï¼š

```python
model.save_pretrained("path/to/somewhere")
```

å®ƒä¿å­˜çš„æ¨¡å‹ç²¾åº¦ä¸ºæ¨¡å‹åœ¨å†…å­˜ä¸­çš„ç²¾åº¦ï¼Œä¸å¯ä»¥è¿›è¡ŒæŒ‡å®š

åç»­å¯ä»¥ç›´æ¥ä½¿ç”¨æ­¤è·¯å¾„è¿›è¡Œæ¨¡å‹çš„åŠ è½½
